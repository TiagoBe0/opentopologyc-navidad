# üî¨ Validaci√≥n Cient√≠fica - Gu√≠a Paso a Paso

**Objetivo:** Entrenar y validar un modelo de predicci√≥n de vacancias at√≥micas con tus datos reales.

---

## üìã Pre-requisitos

Antes de comenzar, aseg√∫rate de tener:

‚úÖ **Datos de entrenamiento:**
- Carpeta con dumps LAMMPS (ej: `db_test_peque√±a/`)
- M√≠nimo 50-100 muestras para entrenamiento decente
- Cada dump debe corresponder a una configuraci√≥n con cierto n√∫mero de vacancias

‚úÖ **Par√°metros del material:**
- Tipo de red cristalina (fcc, bcc, hcp, etc.)
- Par√°metro de red `a0` en √Ö
- N√∫mero de √°tomos en cristal perfecto

‚úÖ **Entorno configurado:**
```bash
pip install -r requirements_qt.txt
```

---

## üöÄ M√©todo R√°pido: quick_train.py

### Paso 1: Configurar Script

Edita `quick_train.py` con tus par√°metros:

```python
# L√≠nea 23: Ruta a tus dumps
DUMP_DIR = "/home/santi-simaf/Documentos/.../db_test_peque√±a"

# L√≠nea 27-31: Par√°metros del material
MATERIAL = {
    "total_atoms": 16384,    # ‚Üê Tu n√∫mero de √°tomos
    "a0": 3.532,             # ‚Üê Tu par√°metro de red
    "lattice_type": "fcc"    # ‚Üê Tu tipo de red
}
```

### Paso 2: Ejecutar

```bash
python quick_train.py
```

### Paso 3: Interpretar Resultados

El script mostrar√°:

```
============================================================
RESULTADOS
============================================================

Accuracy: 0.8542

Classification Report:
              precision    recall  f1-score   support
           0       0.85      0.82      0.84       100
           1       0.88      0.90      0.89       120

Confusion Matrix:
[[82  18]
 [12 108]]

Top 10 Features M√°s Importantes:
  1. hull_volume: 0.2341
  2. grid_count: 0.1892
  ...
```

**Interpretar:**

| Accuracy | Significado | Acci√≥n |
|----------|-------------|--------|
| **> 90%** | üü¢ Excelente | Listo para producci√≥n |
| **70-90%** | üü° Aceptable | Funcional, puede mejorar |
| **< 70%** | üî¥ Bajo | Necesita mejoras |

**Si accuracy < 70%:**
- Agregar m√°s muestras de entrenamiento
- Verificar que labels sean correctos
- Experimentar con `n_estimators=200`

---

## üìä M√©todo Interactivo: validate_system.py

Para m√°s control y opciones:

```bash
python validate_system.py
```

**Men√∫:**
```
1. Extraer features de dumps
2. Entrenar modelo
3. Analizar resultados
4. Informaci√≥n de predicciones
5. Gesti√≥n de modelos (ModelManager)
6. Validaci√≥n completa (1‚Üí2‚Üí3)
0. Salir
```

---

## üéØ Validaci√≥n Cient√≠fica

### 1. Verificar Accuracy

```python
# Objetivo m√≠nimo: 70%
# Objetivo deseable: >85%

if accuracy >= 0.85:
    # Modelo preciso ‚úì
elif accuracy >= 0.70:
    # Modelo funcional, puede mejorar
else:
    # Modelo necesita mejoras
```

### 2. Analizar Confusion Matrix

```
Confusion Matrix:
[[82  18]    ‚Üê Clase 0: 82 correctas, 18 errores
 [12 108]]   ‚Üê Clase 1: 108 correctas, 12 errores
```

**Verificar:**
- Diagonal principal alta (predicciones correctas)
- Fuera de diagonal bajo (errores)
- Sin bias hacia una clase

### 3. Feature Importance

```
Top 10 Features M√°s Importantes:
  1. hull_volume: 0.2341
  2. grid_count: 0.1892
  3. radial_mean: 0.1456
```

**Verificar:**
- Features importantes tengan sentido f√≠sico
- `hull_volume`, `grid_count` suelen ser importantes para vacancias
- Si un feature tiene importancia ~0, puede eliminarse

### 4. Validar F√≠sicamente

**Test cr√≠tico:** ¬øLas predicciones tienen sentido?

```python
# Predecir en dump conocido
dump_con_10_vacancias.dump
‚Üí Predicci√≥n: 9.8 vacancias

# Si predicci√≥n est√° muy lejos (ej: 2 vacancias cuando son 10)
# ‚Üí Modelo tiene problemas, revisar datos
```

---

## üîß Troubleshooting

### Error: "CSV no tiene columna 'label'"

**Problema:** El dataset necesita saber cu√°ntas vacancias tiene cada dump.

**Soluci√≥n 1:** Agregar columna `label` al CSV

```python
import pandas as pd

df = pd.read_csv("dataset_features.csv")

# Si 'n_vacancies' existe, renombrar:
df['label'] = df['n_vacancies']

# O agregar manualmente:
# df['label'] = [0, 1, 2, 3, ...]  # Seg√∫n tus dumps

df.to_csv("dataset_features.csv", index=False)
```

**Soluci√≥n 2:** Parsear desde nombres de archivo

Si tus dumps se llaman `3.6_vac`, `10.2_vac`:

```python
# En core/pipeline.py, l√≠nea 69
import re

filename = Path(file_path).name
match = re.search(r'(\d+\.?\d*)_vac', filename)
if match:
    n_vacancies = int(float(match.group(1)))
else:
    n_vacancies = 0  # Default

feats["label"] = n_vacancies
```

### Error: "Accuracy muy bajo (< 50%)"

**Causas posibles:**

1. **Datos insuficientes:**
   - Necesitas m√≠n. 50-100 muestras
   - Soluci√≥n: Recolectar m√°s dumps

2. **Labels incorrectos:**
   - Verificar que `n_vacancies` sea correcto
   - Soluci√≥n: Validar labels manualmente

3. **Features no informativos:**
   - Algunos features no ayudan a distinguir clases
   - Soluci√≥n: Revisar feature importance

4. **Problema de la tarea:**
   - Tal vez la tarea es muy dif√≠cil
   - Soluci√≥n: Simplificar (ej: clasificar vacancies en rangos)

### Error: "Segmentation Fault"

**Ya est√° solucionado** en la √∫ltima versi√≥n.

Si persiste:
```bash
git pull origin claude/integrate-gui-windows-D2Jbi
```

---

## üìà Mejoras Incrementales

### Experimento 1: M√°s Estimators

```python
# En quick_train.py, l√≠nea 44:
"n_estimators": 200,  # Cambiar de 100 a 200
```

**Efecto:** Modelo m√°s robusto, puede mejorar accuracy 1-3%

### Experimento 2: Max Depth Limitado

```python
# En quick_train.py, l√≠nea 45:
"max_depth": 10,  # Cambiar de None a 10
```

**Efecto:** Previene overfitting, mejor generalizaci√≥n

### Experimento 3: Versionar Modelos

```python
# Entrenar versi√≥n 1.0 con config b√°sica
TRAINING["model_version"] = "1.0"

# Entrenar versi√≥n 2.0 con m√°s estimators
TRAINING["n_estimators"] = 200
TRAINING["model_version"] = "2.0"
```

Luego comparar:
```python
from core.model_manager import ModelManager

manager = ModelManager()
manager.print_summary()
best = manager.compare_models()
```

---

## üéì Criterios de √âxito

Tu modelo est√° **listo para producci√≥n** si:

‚úÖ **Accuracy > 85%** en test set
‚úÖ **Confusion matrix balanceada** (sin bias extremo)
‚úÖ **Features importantes** tienen sentido f√≠sico
‚úÖ **Predicciones validadas** en dumps conocidos
‚úÖ **Reproducible** con ModelManager versionado

---

## üìö Ejemplo Completo

```bash
# 1. Configurar quick_train.py
nano quick_train.py  # Editar DUMP_DIR, MATERIAL

# 2. Entrenar
python quick_train.py

# 3. Ver resultados
# (Se muestran autom√°ticamente)

# 4. Si accuracy > 70%, hacer predicciones
python main_qt.py
# ‚Üí GUI ‚Üí Predicci√≥n ‚Üí Cargar modelo ‚Üí Predecir

# 5. Gestionar modelos
python
>>> from core.model_manager import ModelManager
>>> manager = ModelManager()
>>> manager.print_summary()
```

---

## üöÄ Pr√≥ximos Pasos

Despu√©s de validar con √©xito:

1. **Entrenar versiones mejoradas:**
   - Experimentar con hiperpar√°metros
   - Probar XGBoost / LightGBM (Fase 2)

2. **An√°lisis avanzado:**
   - Cross-validation
   - Learning curves
   - Feature engineering

3. **Producci√≥n:**
   - Integrar en pipeline cient√≠fico
   - Automatizar predicciones
   - Publicar resultados

---

## üìû Soporte

**Documentos relacionados:**
- `ANALISIS_Y_RECOMENDACIONES.md` - Roadmap completo
- `QUICK_WINS_COMPLETADO.md` - Features implementadas
- `README_QT.md` - Uso de la GUI

**Debugging:**
- Ver logs: `cat opentopologyc.log`
- Buscar errores: `grep ERROR opentopologyc.log`

---

**Fecha:** 2026-01-07
**Versi√≥n:** 1.0
**Estado:** ‚úÖ Listo para usar
